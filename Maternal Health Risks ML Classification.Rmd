---
title: "Maternal Health Risks Classification: KNN and Decision Trees"
author: Rex Manglicmot
output: 
  html_notebook: 
    toc: yes
    highlight: pygments
    theme: journal
---
## Status: Continuing Working Document

Hi everyone. Iâ€™m continuing building my data analysis and R skills. As such, I would love feedback to better improve this project via rexmanglicmot@gmail.com. Any mistakes and misrepresentation of the data are my own.

Things Need to Do/Questions:

* provide more context on what is "maternal health risks" according to the site
* rewrite intro and cite sources 
* state objective of this project more convincingly
* provide more concepts/theory about KNN and Decision Trees in each of their sections
* check grammar
* provide more insights into the models
* check validation tests for the moels
* get feedback and incorporate


## Introduction
Maternal health risks has increased since the medieval days. However, there is still a need to understand the underlying factors that contribute to women health includings metrics such as age and blood sugar levels over various age groups.

This projects aims to learn more about such predictors by using machine learning methods such as KNN and Random Forests to see if factors can accurately predict maternal health risks.

Based on UCI's Repository below are the metrics:
1. Age
2. Systolic BP (blood pressure)
3. Diastolic BP (blood pressure)
4. Blood Sugar
5. Body Temp
6. Heart Rate
7. Risk Level

This project is outlined in the following chapters:

1. Loading the Libraries
2. Loading the Data
3. Cleaning the Data
4. Exploratory Data Analysis
5. Modeling: K-Neighrest Neighbors
6. Modeling: Decision Trees
7. Limitations
8. Conclusion
9. Appendix
10. Inspiration for this project

## Loading the Libraries
```{r, message=FALSE}
#load libraries
library(tidyverse)
library(corrplot)
library(RColorBrewer)
library(gghalves)
#library(GGally)
library(class)
library(ggridges)
library(party)
library(partykit)
library(rpart)
library(rpart.plot)
```

## Loading the Data
```{r}
#load data
data <- read.csv('Maternal Health Risk Data Set.csv')
```

## Cleaning the Data
```{r}
#print first few rows of the data
head(data,n=10)

#Change BS to BloodSugar for better understanding
colnames(data)[4] <-'BloodSugar'

#glimpse data
glimpse(data)
```

Now, with the relevant libraries are loaded and the dataset variables are clearly understood, next steps are to clean and check for missing values, characters that are unothordox to each of the metrics via R codes below. 
```{r}
#check for  missing data in full dataset 
#difficult to do when there are lots of observations
sum(is.na(data))

#double check for missing data via different columns
#again difficult
sum(is.na(data$Age))
```
This codes above returns the dataset in which if there are no NA, FALSE will be printed in the whole dataset and the Age column. 

Let's try another method.
```{r}
#check for missing data for each variable column using sum
#output is 0 for all columns
colSums(is.na(data))
```
```{r}
#find location of missing data, if any
which(is.na(data))
```

```{r}
#get percentage of missing data in dataset
mean(is.na(data))
```
The dataset is cleaned and tidy. Next steps are to do EDA.

## Exploratory Data Analysis
```{r}
#Summary statistics of each of the variables
str(data)
```
Many of the variables except RiskLevel are factors and let's see what the distributions are using the summary function. 

```{r}
#Structure of data without the RiskLevel variable because it is a character
summary(data[-7])
```
We see different distributions based on each metric. We see that the SystolicBP and DiastolicBP are normally distributed,and thus nothing interesting to probe with these metrics. The same can also be said about bodyTemp, there is no variance between each summary statistics, further, the regular body temp is ~98 degrees. 

BloodSugar and Heartrate have interesting summary statistics, regarding the max and min, respectively. BloodSugar max value is 19, while the rest of the values are somewhat normally distributed. Heartrate has a min value of 7, while the rest of the values are also somewhat normally distributed. This means that both metrics have outliers, which skews the mean towards the outliers. Unlike HeartRate, the mean of BloodSugar is near the median indicating that the majority of the observatins are near the 6-8 value range. 

Age is interesing because the min and max are 10 and 70, respectively. Generally, as the age increases, there tends to be more complications and thus, this is a variable that I will focus on. 

But first, let's just focus on the RiskLevel variable and see what values are present and each of their count, and visualize the count via a barchart.
```{r}
#what are the of categories within Risk Level
distinct(data,RiskLevel)

#sum categories within Risk Level
data %>%
  group_by(RiskLevel) %>%
  count()

#Visualize count the number of categories within Risk Level
ggplot(data, aes(x=RiskLevel, fill=RiskLevel)) +
  geom_bar(width=.5, color ='black') +
  labs(title = 'Risk Level Bar Chart',
    x= 'Types of Risks',
    y= "Count") +
  scale_fill_brewer(palette = 'RdPu') +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_discrete(limits=c('high risk', 'mid risk', 'low risk')) +
  geom_text(aes(label = ..count..), stat = "count", vjust = 1.5, 
            color = "black")
```
From the above code and bar chart we see that there are three different types of Risk Levels associated with this dataset that the author define as; high, mid, and low risks. From the bar chart above we conclude that the counts are high < mid < low. 

Now, let's focus on exploring the Age variable. 

```{r}
#Histogram of Ages
#it looks bimodal
ggplot(data, aes(Age)) +
  geom_histogram(binwidth=5, fill = 'pink', 
                 color = 'white') +
  theme_bw() +
  ggtitle('Age Distribution of Participants') +
  labs(y = 'Frequency Distribution') +
  theme(plot.title = element_text(hjust = 0.5))
```
A histogram is a means to show the frequency distribution as observations are put into designated bins.

Some insights by ,anipulating the bindwidth, I find the shape of the distribution is *bimodal* signifying that there could be two groups I could possible take a look at. The first and second peak is Age~20 and Age~50. A possible path is that I could break up the dataset into two parts based on the peaks (modes) and each art would have its own summary statistics.

But for now, let's continue with the EDA within age variable. 

```{r}
ggplot(data, aes(x=RiskLevel, y=Age, fill=RiskLevel)) +
  geom_boxplot(width=0.5,
               outlier.color='darkgreen') +
  theme_bw() +
  scale_fill_brewer(palette = 'RdPu') +
  scale_x_discrete(limits=c('high risk', 'mid risk', 'low risk')) +
  ggtitle('Age Distribution based on 
  Risk Level Category') +
  labs(y = 'Age Distribution') +
  theme(plot.title = element_text(hjust = 0.5))
```
Another way is see the distribution is via boxplot. Like the summary() function this plot shows the min, Q1, median, Q3, and max. But unlike that function, the boxplot helps us visualize the data and point out the outliers, which are not shown in the summary table, as shown as green dots here.

Some insights from this graph is that shape of the boxes and whisker lengths are almost identical between the mid and low risk categories. This means that based on this dataset's Age, it is difficult to distinguish between the two. In addition, both have outliers that are above the age, 50. The outliers skews the data, and in this case, pulls the mid and low risk categories upwards to the higher age range. The best way to visualize this is via a density plot, which we will go over in the next section.

The key insight is that of the high risk category boxplot. The "box" which contains 50% of the data is much bigger in comparison to mid and low risk. What this means is that women between the ages of roughly 22-48 could possibly lie within a high risk category of maternal risks. Extrapolating further, the median is also much higher, age~35. What this prompts is that indicating that are women in the mid thirties could possibly lie within a high risk category. This is an interesting finding. 

```{r}
ggplot(data, aes(x=RiskLevel, y=Age, fill=RiskLevel)) +
  geom_half_violin() +
  geom_half_point(alpha=0.2, color='black',
                  size=0.8, position = 'dodge') +
  scale_fill_brewer(palette = 'RdPu') +
  theme_bw() +
  labs(title = 'Half-violin and Half-scatter plots
  of Age based on Risk Level Categories') +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(data, aes(x=Age, y=RiskLevel, fill=RiskLevel)) +
  geom_density_ridges_gradient() +
    scale_fill_brewer(palette = 'RdPu') +
  theme_bw() +
    labs(title = 'Ridge plots
  of Age based on Risk Level Categories') +
  theme(plot.title = element_text(hjust = 0.5))
```
To continue with the Age variable, I wanted to broaden my R skills by using different visualizations the programming language offers. To further compliment the boxplot, I wanted to use gghalves and create a half-violin and half-scatter plot. 

Now, let's move on to a tradtional appraoch to see if there are any correlations between the variables, excluding RiskLevel. One such approach is a correlation matrix that conventiently describes the data, by looking at the raw data.


```{r}
#Create data without last variable that contained chr
# -c(7) corresponds to the last variable
corrplot_data <-data[,-c(7)]

#Check to see if RiskLevel is deleted
names(corrplot_data)

#Simple Corrplot
corrplot(cor(corrplot_data), 
         col=brewer.pal(n=10, name='RdBu'),
         tl.cex = 0.8)
```

## Modeling: K-Neighrest Neighbors
Now, let's see if the dataset metrics can be used to predict whether a person is high, mid, or low risk. The method I'am using is K Nearest Neighbors, a distance algorithmn. In short, it will classify an unknown observation, with it's known metrics, to a category based on it's nearest neigbors 

The data will be split into the traditional 80/20 split that will split 80% of the data into the training set and 20% of the data into the testing set. It is important to note that we never want to train iwth our testing data. 
```{r}
#see structure of the dataset
str(data)

#make a copy of the orginal dataset
data2 <-data
```
We see that dependent variable, RiskLevel, is a character and for purposes of KNN, RiskLevel needs to converted into a factor. 
```{r}
#convert the RiskLevel column, our target, into a factor
data2$RiskLevel <- as.factor(data2$RiskLevel)
```

Now we need to normalize the 6 remaining metrics, but first in order to be more efficient, let's write a function that will iterate over each variable column instead of assigned values witin a variable to an object and piecing it together in another dataframe. Again, we are not interested in the RiskLevel, so I will disregard that variable column when I code the lapply. 
```{r}
#need to normalize data with a range between 0 and 1
#create a function to iterate for each variable column
data_norm <- function (x) {
  ((x-min(x))/ (max(x)- min(x)))
}
#create the normalize data
#lapply will iterate over each of the variables and store it into a 
#dataframe object except for target variable, RiskLevel
data3 <- as.data.frame(lapply(data2[,-7], data_norm))
```

Let's compare the the summary statistics of data2 (pre-normalizaiton) to data3 (post-normalization). 
```{r}
#check to see if ALL values range from 0 to 1
#important to do this before doing KNN
summary(data2[-7])

summary(data3)
```
data 3 is summarized and ready to go into the KNN model. 

Let's do a pre-check when splitting the data into a 80/20 split before actually doing the split. 
```{r}
#create split concept
a <- .8*1014
b <- .2*1014
a+b
811+203
```

## Modeling: Decision Trees

Decision Trees are another way for classification. It is a tree-like flowchart that is used to decide how to classify an obserivation. With every decision there will be a "yes/no" and branch out to the next node and will continue to do so until all the metrics are used. Thus, similar to KNN we will use the metrics to predict the classification a person is one of the 3 RiskLevel types.
```{r}
#set seed for replcation and also no need to normalize the data
set.seed(123)

#split the data
#recall that the dependent variable needs to be a factor, not a chracter so need to use data2 because it is already converted. 
data_dt <- sample(2, nrow(data2), replace= TRUE, prob= c(0.8, 0.2))
data_dt_train <-data2[data_dt==1, ]
data_dt_test <-data2[data_dt==2, ]

#create the decision tree model
#RiskLevel is the dependent variable such that we want to predict
#recall that the "." means everything, so in this case it means all the the variables, 6 total. Using train data
tree <- ctree(RiskLevel~., data_dt_train)

#print tree and it has 27 nodes
print(tree)

#plot the tree
plot(tree)

#use the model on the test data
tree_test_results <- predict(tree, data_dt_test)

#put results into a confusion matrix
table_tree <- table(Predicted= tree_test_results, Acutal= data_dt_test$RiskLevel)

#see the results
table_tree

#Calculate error
1-sum(diag(table_tree))/sum(table_tree)

```
In terms of HighRisk the model predicted 53/61 right. In terms of LowRisk the model predicted 72/82 right. In terms of MidRisk, the model predicted 13/56 right. 

The miscalculation is 31% meaning the accuracy of the model is 78%.
```{r}
#lets try another plotting format
tree2 <-rpart(RiskLevel~., data_dt_train)
rpart.plot(tree2)
## Need to figure out how to interpret this
```

```{r}
#create 80/20 split for testing and training set based on the rows
data3_train <- data3[1:811, ]
data3_test <- data3[812:1014, ]

#create the pred model
data3_pred <- knn(
  data3_train,
  data3_test,
  #create training labels but labels are present in data2 and on RiskLevel,
  #which is column 7
  data2[1:811,7],
  #need to specify value of K
  #general rule of thumb is sq.rt. of 1014 is 31-ish
  k=31)

#validate pred labels with the actual labels on the RiskLevel, which is column 7
table_knn <- table(Predicted = data3_pred, Actual = data2[812:1014,7])

#view table
table_knn

#Calculate error
1-sum(diag(table_knn))/sum(table_knn)
```

The miscalculation is 38% meaning the accuracy of the model is 62%. This is not optimal.

## Limitations


## Conclusion
I will finish this part when the above sections are completed. 

## Appendix
```{r}
#Scatterplot for Age and BS
#Not much of a realtionship
ggplot(data, aes(Age, BloodSugar)) +
  geom_point(color='lightpink') +
  geom_smooth() +
  theme_bw()
```

## Inspiration for this project

